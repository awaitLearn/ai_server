import torch
from diffusers import StableVideoDiffusionPipeline, AnimateDiffPipeline, StableDiffusionXLPipeline
from PIL import Image
import cv2
import numpy as np
import time

class LocalVideoGenerator:
    def __init__(self, model_type="svd"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_type = model_type
        
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_type}...")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        if self.model_type == "svd":
            self.pipeline = StableVideoDiffusionPipeline.from_pretrained(
                "stabilityai/stable-video-diffusion-img2vid-xt",
                torch_dtype=torch.float16,
                variant="fp16"
            ).to(self.device)
            
        elif self.model_type == "animatediff":
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SDXL –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤
            self.sdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16
            ).to(self.device)
            
            # –ò AnimateDiff –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
            self.animate_pipeline = AnimateDiffPipeline.from_pretrained(
                "emilianJR/epiCRealism",
                torch_dtype=torch.float16
            ).to(self.device)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def generate_from_image(self, image_path: str, prompt: str, duration: int = 10) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        print(f"üé¨ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {duration} —Å–µ–∫ –≤–∏–¥–µ–æ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.open(image_path).convert("RGB")
        image = image.resize((1024, 576))  # 16:9
        
        if self.model_type == "svd":
            return self._generate_svd(image, duration)
        else:
            return self._generate_animatediff(image, prompt, duration)

    def _generate_svd(self, image: Image.Image, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Stable Video Diffusion"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞–¥—Ä—ã (25fps)
        num_frames = min(100, duration * 25)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
        video_frames = self.pipeline(
            image,
            num_frames=num_frames,
            num_inference_steps=25,
            motion_bucket_id=180,
            fps=25
        ).frames[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = f"svd_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=25)
        
        return output_path

    def _generate_animatediff(self, image: Image.Image, prompt: str, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ AnimateDiff + SDXL"""
        # –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä —á–µ—Ä–µ–∑ SDXL
        print("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä...")
        base_frame = self.sdxl_pipeline(
            prompt=prompt,
            image=image,
            strength=0.7,
            num_inference_steps=30
        ).images[0]
        
        # –ó–∞—Ç–µ–º –∞–Ω–∏–º–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ AnimateDiff
        print("üé¨ –ê–Ω–∏–º–∏—Ä—É–µ–º –∫–∞–¥—Ä...")
        num_frames = min(120, duration * 24)
        
        video_frames = self.animate_pipeline(
            prompt=prompt,
            image=base_frame,
            num_frames=num_frames,
            num_inference_steps=25,
            fps=24
        ).frames[0]
        
        output_path = f"animatediff_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=24)
        
        return output_path

    def _save_video(self, frames, output_path: str, fps: int = 24):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä—ã –∫–∞–∫ –≤–∏–¥–µ–æ"""
        if not frames:
            return None
        
        height, width = frames[0].size
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        for frame in frames:
            out.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))
        
        out.release()
        return output_path

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = LocalVideoGenerator(model_type="svd")  # –∏–ª–∏ "animatediff"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
    result = generator.generate_from_image(
        image_path="face.jpg",
        prompt="beautiful man, cinematic portrait, high quality",
        duration=10  # —Å–µ–∫—É–Ω–¥
    )
    
    print(f"‚úÖ –í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ: {result}")