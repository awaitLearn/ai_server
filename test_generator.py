import torch
from diffusers import StableVideoDiffusionPipeline, AnimateDiffPipeline, StableDiffusionXLPipeline
from PIL import Image
import cv2
import numpy as np
import time
import gc
import os
import imageio

class LocalVideoGenerator:
    def __init__(self, model_type="svd"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_type = model_type
        
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_type}...")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        if self.model_type == "svd":
            self.pipeline = StableVideoDiffusionPipeline.from_pretrained(
                "stabilityai/stable-video-diffusion-img2vid-xt",
                torch_dtype=torch.float16,
                variant="fp16"
            ).to(self.device)
            
        elif self.model_type == "animatediff":
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SDXL –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤
            self.sdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16
            ).to(self.device)
            
            # –ò AnimateDiff –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
            self.animate_pipeline = AnimateDiffPipeline.from_pretrained(
                "emilianJR/epiCRealism",
                torch_dtype=torch.float16
            ).to(self.device)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ CUDA"""
        torch.cuda.empty_cache()
        gc.collect()
        print("üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")

    def generate_from_image(self, image_path: str, prompt: str, duration: int = 10) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        print(f"üé¨ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {duration} —Å–µ–∫ –≤–∏–¥–µ–æ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.open(image_path).convert("RGB")
        image = image.resize((1024, 576))  # 16:9
        
        try:
            if self.model_type == "svd":
                return self._generate_svd(image, duration)
            else:
                return self._generate_animatediff(image, prompt, duration)
        finally:
            self._clear_memory()

    def _generate_svd(self, image: Image.Image, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Stable Video Diffusion"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞–¥—Ä—ã (—É–º–µ–Ω—å—à–∞–µ–º FPS –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
        num_frames = min(75, duration * 15)  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 25 –¥–æ 15 FPS
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        video_frames = self.pipeline(
            image,
            num_frames=num_frames,
            num_inference_steps=20,  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
            motion_bucket_id=150,    # –£–º–µ–Ω—å—à–∏–ª–∏ motion –¥–ª—è –º–µ–Ω—å—à–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            fps=15,                  # –£–º–µ–Ω—å—à–∏–ª–∏ FPS
            decode_chunk_size=4,     # –î–æ–±–∞–≤–∏–º chunking –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            generator=torch.Generator(self.device).manual_seed(int(time.time()))
        ).frames[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = f"svd_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=15)
        
        return output_path

    def _generate_animatediff(self, image: Image.Image, prompt: str, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ AnimateDiff + SDXL"""
        # –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä —á–µ—Ä–µ–∑ SDXL
        print("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä...")
        base_frame = self.sdxl_pipeline(
            prompt=prompt,
            image=image,
            strength=0.6,           # –£–º–µ–Ω—å—à–∏–ª–∏ strength
            num_inference_steps=25,  # –£–º–µ–Ω—å—à–∏–ª–∏ —à–∞–≥–∏
            guidance_scale=7.0       # –£–º–µ–Ω—å—à–∏–ª–∏ guidance scale
        ).images[0]
        
        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–∞
        self._clear_memory()
        
        # –ó–∞—Ç–µ–º –∞–Ω–∏–º–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ AnimateDiff
        print("üé¨ –ê–Ω–∏–º–∏—Ä—É–µ–º –∫–∞–¥—Ä...")
        num_frames = min(80, duration * 20)  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        
        video_frames = self.animate_pipeline(
            prompt=prompt,
            image=base_frame,
            num_frames=num_frames,
            num_inference_steps=20,  # –£–º–µ–Ω—å—à–∏–ª–∏ —à–∞–≥–∏
            fps=20,                  # –£–º–µ–Ω—å—à–∏–ª–∏ FPS
            guidance_scale=7.0       # –£–º–µ–Ω—å—à–∏–ª–∏ guidance scale
        ).frames[0]
        
        output_path = f"animatediff_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=20)
        
        return output_path

    def _save_video(self, frames, output_path: str, fps: int = 24):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä—ã –∫–∞–∫ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é imageio"""
        if not frames:
            print("‚ùå –ù–µ—Ç –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return None
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL Image –≤ numpy arrays
            print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ...")
            frames_np = [np.array(frame) for frame in frames]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–º–æ—â—å—é imageio
            with imageio.get_writer(output_path, fps=fps, codec='libx264') as writer:
                for frame in frames_np:
                    writer.append_data(frame)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                print(f"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path} ({file_size} bytes)")
                return output_path
            else:
                print("‚ùå –§–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–ª—Å—è")
                return None
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            
            # Fallback: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ PNG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä—ã –∫–∞–∫ PNG...")
            os.makedirs("frames", exist_ok=True)
            for i, frame in enumerate(frames):
                frame_path = f"frames/frame_{i:04d}.png"
                frame.save(frame_path)
            print("‚úÖ –ö–∞–¥—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É 'frames/'")
            return "frames/"

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = LocalVideoGenerator(model_type="svd")  # –∏–ª–∏ "animatediff"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
    result = generator.generate_from_image(
        image_path="face.jpg",
        prompt="I leave the room and then find myself in the world of ANIME NARUTO, and start fighting with enemies in a beautiful field with flowers in rainy weather",
        duration=30  # —Å–µ–∫—É–Ω–¥
    )
    
    print(f"‚úÖ –í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ: {result}")

