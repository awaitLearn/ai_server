import torch
from diffusers import StableVideoDiffusionPipeline, AnimateDiffPipeline, StableDiffusionXLPipeline
from PIL import Image
import cv2
import numpy as np
import time
import gc

class LocalVideoGenerator:
    def __init__(self, model_type="svd"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_type = model_type
        
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_type}...")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        if self.model_type == "svd":
            self.pipeline = StableVideoDiffusionPipeline.from_pretrained(
                "stabilityai/stable-video-diffusion-img2vid-xt",
                torch_dtype=torch.float16,
                variant="fp16"
            ).to(self.device)
            
        elif self.model_type == "animatediff":
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SDXL –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤
            self.sdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16
            ).to(self.device)
            
            # –ò AnimateDiff –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
            self.animate_pipeline = AnimateDiffPipeline.from_pretrained(
                "emilianJR/epiCRealism",
                torch_dtype=torch.float16
            ).to(self.device)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ CUDA"""
        torch.cuda.empty_cache()
        gc.collect()
        print("üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")

    def generate_from_image(self, image_path: str, prompt: str, duration: int = 10) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        print(f"üé¨ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º {duration} —Å–µ–∫ –≤–∏–¥–µ–æ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.open(image_path).convert("RGB")
        image = image.resize((1024, 576))  # 16:9
        
        try:
            if self.model_type == "svd":
                return self._generate_svd(image, duration)
            else:
                return self._generate_animatediff(image, prompt, duration)
        finally:
            self._clear_memory()

    def _generate_svd(self, image: Image.Image, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Stable Video Diffusion"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∫–∞–¥—Ä—ã (—É–º–µ–Ω—å—à–∞–µ–º FPS –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
        num_frames = min(75, duration * 15)  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 25 –¥–æ 15 FPS
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        video_frames = self.pipeline(
            image,
            num_frames=num_frames,
            num_inference_steps=20,  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
            motion_bucket_id=150,    # –£–º–µ–Ω—å—à–∏–ª–∏ motion –¥–ª—è –º–µ–Ω—å—à–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            fps=15,                  # –£–º–µ–Ω—å—à–∏–ª–∏ FPS
            decode_chunk_size=4,     # –î–æ–±–∞–≤–∏–º chunking –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            generator=torch.Generator(self.device).manual_seed(int(time.time()))
        ).frames[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = f"svd_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=15)
        
        return output_path

    def _generate_animatediff(self, image: Image.Image, prompt: str, duration: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ AnimateDiff + SDXL"""
        # –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä —á–µ—Ä–µ–∑ SDXL
        print("üé® –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∫–∞–¥—Ä...")
        base_frame = self.sdxl_pipeline(
            prompt=prompt,
            image=image,
            strength=0.6,           # –£–º–µ–Ω—å—à–∏–ª–∏ strength
            num_inference_steps=25,  # –£–º–µ–Ω—å—à–∏–ª–∏ —à–∞–≥–∏
            guidance_scale=7.0       # –£–º–µ–Ω—å—à–∏–ª–∏ guidance scale
        ).images[0]
        
        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–∞
        self._clear_memory()
        
        # –ó–∞—Ç–µ–º –∞–Ω–∏–º–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ AnimateDiff
        print("üé¨ –ê–Ω–∏–º–∏—Ä—É–µ–º –∫–∞–¥—Ä...")
        num_frames = min(80, duration * 20)  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        
        video_frames = self.animate_pipeline(
            prompt=prompt,
            image=base_frame,
            num_frames=num_frames,
            num_inference_steps=20,  # –£–º–µ–Ω—å—à–∏–ª–∏ —à–∞–≥–∏
            fps=20,                  # –£–º–µ–Ω—å—à–∏–ª–∏ FPS
            guidance_scale=7.0       # –£–º–µ–Ω—å—à–∏–ª–∏ guidance scale
        ).frames[0]
        
        output_path = f"animatediff_video_{int(time.time())}.mp4"
        self._save_video(video_frames, output_path, fps=20)
        
        return output_path

    def _save_video(self, frames, output_path: str, fps: int = 24):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä—ã –∫–∞–∫ –≤–∏–¥–µ–æ"""
        if not frames:
            return None
        
        height, width = frames[0].size
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        for frame in frames:
            out.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))
        
        out.release()
        return output_path

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = LocalVideoGenerator(model_type="svd")  # –∏–ª–∏ "animatediff"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
    result = generator.generate_from_image(
        image_path="face.jpg",
        prompt="beautiful man, cinematic portrait, high quality",
        duration=10  # —Å–µ–∫—É–Ω–¥
    )
    
    print(f"‚úÖ –í–∏–¥–µ–æ –≥–æ—Ç–æ–≤–æ: {result}")
